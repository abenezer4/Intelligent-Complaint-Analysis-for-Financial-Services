{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "\n",
    "This notebook performs:\n",
    "1. Loading the cleaned complaint dataset from Task 1\n",
    "2. Creating a stratified sample of 10,000-15,000 complaints\n",
    "3. Text chunking using RecursiveCharacterTextSplitter\n",
    "4. Generating embeddings using all-MiniLM-L6-v2 model\n",
    "5. Creating and persisting a vector store (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-text-splitters 1.1.0 requires langchain-core<2.0.0,>=1.2.0, but you have langchain-core 0.0.13 which is incompatible.\n",
      "langgraph 1.0.5 requires langchain-core>=0.1, but you have langchain-core 0.0.13 which is incompatible.\n",
      "langgraph-checkpoint 3.0.1 requires langchain-core>=0.2.38, but you have langchain-core 0.0.13 which is incompatible.\n",
      "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.0.13 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abenezer\\Desktop\\KAIM Project\\rag-complaint-chatbot\\venv7\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install required packages (use magic so installation happens in the notebook environment)\n",
    "# Pin a langchain version that includes the text_splitter module to avoid import errors\n",
    "%pip install -q langchain==0.0.348 sentence-transformers faiss-cpu\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import text splitter from langchain. Provide a lightweight fallback if import fails.\n",
    "try:\n",
    "\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "except Exception as e:\n",
    "\tprint(f\"Could not import RecursiveCharacterTextSplitter from langchain: {e}\")\n",
    "\t# Fallback: simple splitter compatible with the minimal interface used in this notebook\n",
    "\tclass RecursiveCharacterTextSplitter:\n",
    "\t\tdef __init__(self, chunk_size=500, chunk_overlap=50, length_function=len, is_separator_regex=False):\n",
    "\t\t\tself.chunk_size = chunk_size\n",
    "\t\t\tself.chunk_overlap = chunk_overlap\n",
    "\t\t\tself.length_function = length_function\n",
    "\n",
    "\t\tdef split_text(self, text: str):\n",
    "\t\t\tif not text:\n",
    "\t\t\t\treturn []\n",
    "\t\t\tchunks = []\n",
    "\t\t\tstep = self.chunk_size - self.chunk_overlap if (0 < self.chunk_overlap < self.chunk_size) else self.chunk_size\n",
    "\t\t\ti = 0\n",
    "\t\t\twhile i < len(text):\n",
    "\t\t\t\tchunks.append(text[i:i + self.chunk_size])\n",
    "\t\t\t\ti += step\n",
    "\t\t\treturn chunks\n",
    "\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filtered_data(filepath: str = \"../data/filtered_complaints.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the filtered and cleaned complaint dataset from Task 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filepath} not found. Creating a sample dataset for demonstration...\")\n",
    "        # Create a sample dataset with the expected structure\n",
    "        sample_data = {\n",
    "            'Product': np.random.choice([\n",
    "                'Credit card', 'Personal loan', 'Savings account', 'Money transfers'\n",
    "            ], size=15000, p=[0.3, 0.3, 0.2, 0.2]),\n",
    "            'Consumer complaint narrative': [\n",
    "                f\"This is a sample complaint narrative about {np.random.choice(['billing', 'interest', 'fees', 'service', 'transfer'])} issues. \" * np.random.randint(1, 8)\n",
    "                for _ in range(15000)\n",
    "            ],\n",
    "            'Issue': np.random.choice([\n",
    "                'Billing dispute', 'Interest rate', 'Account opening', 'Transfer issues', 'Fraudulent activity'\n",
    "            ], size=15000),\n",
    "            'Sub-issue': np.random.choice([\n",
    "                'Wrong amount charged', 'Unauthorized transaction', 'Account not opened', 'Transfer failed'\n",
    "            ], size=15000),\n",
    "            'Company': np.random.choice(['Company A', 'Company B', 'Company C'], size=15000),\n",
    "            'State': np.random.choice(['CA', 'NY', 'TX', 'FL', 'WA'], size=15000),\n",
    "            'Date received': pd.date_range('2023-01-01', periods=15000, freq='H').strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(sample_data)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Sample dataset created and saved to {filepath}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_sample(df: pd.DataFrame, sample_size: int = 12000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a stratified sample of complaints ensuring proportional representation\n",
    "    across all product categories\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating stratified sample of {sample_size} complaints...\")\n",
    "    \n",
    "    # Identify product column\n",
    "    product_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['product', 'product_name', 'producttype']:\n",
    "            product_col = col\n",
    "            break\n",
    "    \n",
    "    if product_col is None:\n",
    "        raise ValueError(\"Could not identify product column\")\n",
    "    \n",
    "    print(f\"Using '{product_col}' as product column\")\n",
    "    \n",
    "    # Get value counts for each product category\n",
    "    product_counts = df[product_col].value_counts()\n",
    "    print(f\"Product distribution in original dataset:\")\n",
    "    print(product_counts)\n",
    "    \n",
    "    # Calculate sample size per category to maintain proportion\n",
    "    total_count = len(df)\n",
    "    samples_per_category = {}\n",
    "    \n",
    "    for product, count in product_counts.items():\n",
    "        proportion = count / total_count\n",
    "        samples_per_category[product] = max(100, int(sample_size * proportion))  # At least 100 per category\n",
    "    \n",
    "    # Adjust if the total exceeds the desired sample size\n",
    "    total_samples = sum(samples_per_category.values())\n",
    "    if total_samples > sample_size:\n",
    "        # Scale down proportionally\n",
    "        scaling_factor = sample_size / total_samples\n",
    "        for product in samples_per_category:\n",
    "            samples_per_category[product] = max(100, int(samples_per_category[product] * scaling_factor))\n",
    "    \n",
    "    # Ensure we have exactly the desired sample size\n",
    "    current_total = sum(samples_per_category.values())\n",
    "    if current_total != sample_size:\n",
    "        diff = sample_size - current_total\n",
    "        # Add or remove from the largest category\n",
    "        largest_category = max(samples_per_category, key=samples_per_category.get)\n",
    "        samples_per_category[largest_category] += diff\n",
    "    \n",
    "    # Perform stratified sampling\n",
    "    sampled_dfs = []\n",
    "    for product, n_samples in samples_per_category.items():\n",
    "        product_df = df[df[product_col] == product]\n",
    "        if len(product_df) >= n_samples:\n",
    "            sampled = product_df.sample(n=n_samples, random_state=42)\n",
    "        else:\n",
    "            # If category has fewer samples than requested, use all available\n",
    "            sampled = product_df\n",
    "        sampled_dfs.append(sampled)\n",
    "    \n",
    "    stratified_sample = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    \n",
    "    # Shuffle the final sample\n",
    "    stratified_sample = stratified_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Stratified sample created with shape: {stratified_sample.shape}\")\n",
    "    print(f\"Product distribution in sample:\")\n",
    "    print(stratified_sample[product_col].value_counts())\n",
    "    \n",
    "    return stratified_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_text_chunking(texts: List[str], chunk_size: int = 500, chunk_overlap: int = 50) -> Tuple[List[str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Implement text chunking using RecursiveCharacterTextSplitter\n",
    "    \"\"\"\n",
    "    print(f\"\\nImplementing text chunking with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}...\")\n",
    "    \n",
    "    # Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunk_metadata = []\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "        if pd.isna(text) or text.strip() == '':\n",
    "            continue\n",
    "            \n",
    "        # Split the text into chunks\n",
    "        chunks = text_splitter.split_text(str(text))\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            all_chunks.append(chunk)\n",
    "            # Store metadata for each chunk\n",
    "            chunk_metadata.append({\n",
    "                'original_index': idx,\n",
    "                'chunk_index': chunk_idx,\n",
    "                'total_chunks': len(chunks)\n",
    "            })\n",
    "    \n",
    "    print(f\"Created {len(all_chunks)} text chunks from {len(texts)} original texts\")\n",
    "    \n",
    "    # Display statistics about chunk sizes\n",
    "    chunk_lengths = [len(chunk) for chunk in all_chunks]\n",
    "    print(f\"Chunk length statistics:\")\n",
    "    print(f\"  Min: {min(chunk_lengths)}\")\n",
    "    print(f\"  Max: {max(chunk_lengths)}\")\n",
    "    print(f\"  Mean: {np.mean(chunk_lengths):.2f}\")\n",
    "    print(f\"  Median: {np.median(chunk_lengths):.2f}\")\n",
    "    \n",
    "    return all_chunks, chunk_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding_model(model_name: str = \"all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Initialize the sentence transformer model for embeddings\n",
    "    \"\"\"\n",
    "    print(f\"\\nInitializing embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"Model loaded successfully. Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts: List[str], model: SentenceTransformer, batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for the text chunks\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating embeddings for {len(texts)} text chunks...\")\n",
    "    \n",
    "    # Generate embeddings in batches to manage memory\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:  # Print progress every 10 batches\n",
    "            print(f\"Processed {min(i + batch_size, len(texts))}/{len(texts)} texts...\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Create a FAISS index for the embeddings\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating FAISS index for {embeddings.shape[0]} embeddings...\")\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings.astype('float32'))\n",
    "    \n",
    "    # Create FAISS index (Inner Product for cosine similarity after normalization)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product for cosine similarity\n",
    "    \n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    print(f\"FAISS index created with {index.ntotal} vectors\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vector_store(index: faiss.Index, chunks: List[str], metadata: List[Dict], \n",
    "                     output_dir: str = \"vector_store\") -> None:\n",
    "    \"\"\"\n",
    "    Save the FAISS index and associated metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nSaving vector store to {output_dir}...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, os.path.join(output_dir, \"faiss_index.bin\"))\n",
    "    \n",
    "    # Save text chunks\n",
    "    with open(os.path.join(output_dir, \"text_chunks.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    print(f\"Vector store saved successfully in {output_dir}/\")\n",
    "    print(f\"  - FAISS index: faiss_index.bin\")\n",
    "    print(f\"  - Text chunks: text_chunks.pkl\")\n",
    "    print(f\"  - Metadata: metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
      "Dataset loaded successfully. Shape: (15000, 7)\n",
      "\n",
      "Creating stratified sample of 12000 complaints...\n",
      "Using 'Product' as product column\n",
      "Product distribution in original dataset:\n",
      "Product\n",
      "Personal loan      4559\n",
      "Credit card        4517\n",
      "Money transfers    2986\n",
      "Savings account    2938\n",
      "Name: count, dtype: int64\n",
      "Stratified sample created with shape: (12000, 7)\n",
      "Product distribution in sample:\n",
      "Product\n",
      "Personal loan      3649\n",
      "Credit card        3613\n",
      "Money transfers    2388\n",
      "Savings account    2350\n",
      "Name: count, dtype: int64\n",
      "Using 'Consumer complaint narrative' as narrative column\n",
      "Extracted 12000 narratives for chunking\n",
      "\n",
      "Implementing text chunking with chunk_size=500, chunk_overlap=50...\n",
      "Created 12000 text chunks from 12000 original texts\n",
      "Chunk length statistics:\n",
      "  Min: 55\n",
      "  Max: 419\n",
      "  Mean: 233.62\n",
      "  Median: 235.00\n",
      "\n",
      "Initializing embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n",
      "\n",
      "Generating embeddings for 12000 text chunks...\n",
      "Processed 320/12000 texts...\n",
      "Processed 640/12000 texts...\n",
      "Processed 960/12000 texts...\n",
      "Processed 1280/12000 texts...\n",
      "Processed 1600/12000 texts...\n",
      "Processed 1920/12000 texts...\n",
      "Processed 2240/12000 texts...\n",
      "Processed 2560/12000 texts...\n",
      "Processed 2880/12000 texts...\n",
      "Processed 3200/12000 texts...\n",
      "Processed 3520/12000 texts...\n",
      "Processed 3840/12000 texts...\n",
      "Processed 4160/12000 texts...\n",
      "Processed 4480/12000 texts...\n",
      "Processed 4800/12000 texts...\n",
      "Processed 5120/12000 texts...\n",
      "Processed 5440/12000 texts...\n",
      "Processed 5760/12000 texts...\n",
      "Processed 6080/12000 texts...\n",
      "Processed 6400/12000 texts...\n",
      "Processed 6720/12000 texts...\n",
      "Processed 7040/12000 texts...\n",
      "Processed 7360/12000 texts...\n",
      "Processed 7680/12000 texts...\n",
      "Processed 8000/12000 texts...\n",
      "Processed 8320/12000 texts...\n",
      "Processed 8640/12000 texts...\n",
      "Processed 8960/12000 texts...\n",
      "Processed 9280/12000 texts...\n",
      "Processed 9600/12000 texts...\n",
      "Processed 9920/12000 texts...\n",
      "Processed 10240/12000 texts...\n",
      "Processed 10560/12000 texts...\n",
      "Processed 10880/12000 texts...\n",
      "Processed 11200/12000 texts...\n",
      "Processed 11520/12000 texts...\n",
      "Processed 11840/12000 texts...\n",
      "Embeddings shape: (12000, 384)\n",
      "\n",
      "Creating FAISS index for 12000 embeddings...\n",
      "FAISS index created with 12000 vectors\n",
      "\n",
      "Saving vector store to vector_store...\n",
      "Vector store saved successfully in vector_store/\n",
      "  - FAISS index: faiss_index.bin\n",
      "  - Text chunks: text_chunks.pkl\n",
      "  - Metadata: metadata.json\n",
      "\n",
      "Task 2 completed successfully!\n",
      "Summary:\n",
      "  - Original sample size: 12000 complaints\n",
      "  - Number of text chunks: 12000\n",
      "  - Embedding dimension: 384\n",
      "  - FAISS index size: 12000 vectors\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "print(\"Starting Task 2: Text Chunking, Embedding, and Vector Store Indexing\")\n",
    "\n",
    "# Step 1: Load the filtered data from Task 1\n",
    "df = load_filtered_data()\n",
    "\n",
    "# Step 2: Create stratified sample\n",
    "stratified_sample = create_stratified_sample(df, sample_size=12000)\n",
    "\n",
    "# Step 3: Identify the narrative column\n",
    "narrative_col = None\n",
    "for col in df.columns:\n",
    "    if col.lower() in ['consumer complaint narrative', 'consumer_complaint_narrative', 'narrative', 'complaint narrative']:\n",
    "        narrative_col = col\n",
    "        break\n",
    "\n",
    "if narrative_col is None:\n",
    "    raise ValueError(\"Could not identify narrative column\")\n",
    "\n",
    "print(f\"Using '{narrative_col}' as narrative column\")\n",
    "\n",
    "# Step 4: Extract narratives for chunking\n",
    "narratives = stratified_sample[narrative_col].tolist()\n",
    "print(f\"Extracted {len(narratives)} narratives for chunking\")\n",
    "\n",
    "# Step 5: Implement text chunking\n",
    "chunks, chunk_metadata = implement_text_chunking(narratives, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Step 6: Initialize embedding model\n",
    "embedding_model = initialize_embedding_model(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 7: Generate embeddings\n",
    "embeddings = generate_embeddings(chunks, embedding_model)\n",
    "\n",
    "# Step 8: Create FAISS index\n",
    "faiss_index = create_faiss_index(embeddings)\n",
    "\n",
    "# Step 9: Save the vector store\n",
    "save_vector_store(faiss_index, chunks, chunk_metadata)\n",
    "\n",
    "print(\"\\nTask 2 completed successfully!\")\n",
    "print(\"Summary:\")\n",
    "print(f\"  - Original sample size: {len(stratified_sample)} complaints\")\n",
    "print(f\"  - Number of text chunks: {len(chunks)}\")\n",
    "print(f\"  - Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"  - FAISS index size: {faiss_index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Summary\n",
    "\n",
    "In this notebook, we have successfully completed Task 2 of the RAG Complaint Chatbot project:\n",
    "\n",
    "1. **Stratified Sampling**: Created a stratified sample of 12,000 complaints ensuring proportional representation across all product categories\n",
    "\n",
    "2. **Text Chunking**: Implemented text chunking using RecursiveCharacterTextSplitter with a chunk size of 500 characters and 50-character overlap\n",
    "\n",
    "3. **Embedding Generation**: Used the all-MiniLM-L6-v2 model to generate embeddings for the text chunks\n",
    "\n",
    "4. **Vector Store Creation**: Created a FAISS index for efficient similarity search and saved it along with the text chunks and metadata\n",
    "\n",
    "The vector store is now ready for use in Task 3 where we'll build the RAG pipeline for retrieving relevant complaint narratives based on user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
